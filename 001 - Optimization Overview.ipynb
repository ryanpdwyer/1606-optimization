{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General optimization problem\n",
    "\n",
    "- Consider minimizing $f(x)$, where $x$ is a vector of $N$ parameters and $f$ a scalar function.\n",
    "- At a local minimum $x^*$,\n",
    "    - The gradient of $f$ is zero.\n",
    "       $$\\nabla f(x^*) = 0 = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_N}\n",
    "\\end{pmatrix},$$\n",
    "    - The Hessian (matrix of second derivatives) is positive semidefinite. This distinguishes local minima from saddle points, local maxima.\n",
    "- Focus on **local optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods\n",
    "\n",
    "Iterative methods are necessary for large / non-linear problems.\n",
    "\n",
    "- Start with an initial guess $x_0$\n",
    "- Generate successive guesses $x_k$ that converge to mimimum.\n",
    "- Many different methods, 2 general strategies:\n",
    "    - **Line search:** Choose a direction, then take a step in that direction which minimizes / decreases $f$\n",
    "    - **Trust region:** Choose a region around the current $x$ where $f$ can be well-approximated, take a step which stays within the region and minimizes / decreases $f$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Picture\n",
    "\n",
    "<img alt=\"Optimization in 1D\" src=\"newton-picture.png\" width=820px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "- The first derivative, or gradient $\\nabla f$ is the most valuable piece of information\n",
    "    - Almost every method uses $\\nabla f$, calculated analytically, using finite differences, or automatic differentiation\n",
    "    - However, $\\nabla f$ alone gives no information on appropriate step size\n",
    "- The 2nd derivative of $f$, or Hessian matrix $H$ is also very useful\n",
    "    $$H = \\begin{pmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2}  & \\cdots  & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_N} \\\\\n",
    "\\vdots &  \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_N \\partial x_1}&  \\cdots & \\frac{\\partial^2 f}{\\partial x_N^2} \n",
    "\\end{pmatrix}$$\n",
    "    - Hessian is the quadratic term of a local Taylor expansion\n",
    "    - Allows natural Newton step,\n",
    "        $$x_{k+1} = x_k - H^{-1}(x_k) \\nabla f(x_k)$$\n",
    "        the exact minimum of the local Taylor expansion.\n",
    "    - Can enable much faster convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple step strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "<img alt=\"Gradient descent\" src=\"http://trond.hjorteland.com/thesis/img200.gif\" width=480px>\n",
    "\n",
    "- Take steps in the direction of steepest descent\n",
    "    $$x_{k+1} = x_k - \\alpha \\nabla f(x_k)$$\n",
    "- Many ways to determine the step size $\\alpha$\n",
    "- **Guaranteed convergence to local minimum** (for most reasonable choices of $\\alpha$)\n",
    "\n",
    "**Problem: Slow convergence!**\n",
    "\n",
    "<img alt=\"Gradient descent issues\" src=\"http://trond.hjorteland.com/thesis/img208.gif\" width=480px>\n",
    "\n",
    "- Sensitive to problem scaling\n",
    "- Narrow ridges slow convergence\n",
    "- Converges linearly, so can require many iterations to obtain required accuracy\n",
    "\n",
    "**There are usually much better choices than gradient descent.**\n",
    "\n",
    "Images from [Trond Hjorteland's thesis](http://trond.hjorteland.com/thesis/node26.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "Take Newton steps,\n",
    "$$x_{k+1} = x_k - H^{-1}(x_k) \\nabla f(x_k)$$\n",
    "\n",
    "**Advantages**\n",
    "- Close to the minimum, converges quadratically (steepest descent converges linearly)\n",
    "- Less sensitive to problem scaling\n",
    "\n",
    "**Disadvantages**\n",
    "- Not guaranteed to converge\n",
    "- Requires inverting the Hessian, extremely computationally intensive for large problems\n",
    "\n",
    "- Many methods approximate the Hessian, see [Quasi-Newtown methods](https://en.wikipedia.org/wiki/Quasi-Newton_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- *Numerical Optimization*, Nocedal, J. & Wright, S. [doi: 10.1007/978-0-387-40065-5](http://dx.doi.org/10.1007/978-0-387-40065-5)\n",
    "- *Linear algebra and its applications*, Strang, G. ISBN: 0030105676\n",
    "- *Convex Optimization*, Boyd, S. & Vandenberghe, L. (ISBN: 0521833787, available at [http://stanford.edu/~boyd/cvxbook/](http://stanford.edu/~boyd/cvxbook/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
